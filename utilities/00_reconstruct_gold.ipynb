{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7e4a85-e723-4860-af50-934425f0a5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This ugly crap is needed to import modules from the parent folder\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(f\"{os.getcwd()}/..\")\n",
    "\n",
    "from pyspark.sql.functions import to_date, from_utc_timestamp, current_timestamp, lit, sha2, to_json, struct, col\n",
    "from functions import create_table_if_not_exists\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "settings = {\n",
    "    \"pipeline_function\": \"gold.powerPlay\",\n",
    "    \"src_table_name\": \"edsm.silver.powerPlay\",\n",
    "    \"dst_table_name\": \"edsm.gold.powerPlay\",\n",
    "    \"merge_condition\": \"t.name = s.name and t.id = s.id and t.id64 = s.id64 and t.power = s.power\",\n",
    "    \"readStreamOptions\": {\n",
    "        \"rescuedDataColumn\": \"_rescued_data\",\n",
    "        \"ignoreChanges\": \"true\"\n",
    "    },\n",
    "    \"writeStreamOptions\": {\n",
    "        \"checkpointLocation\": \"/Volumes/edsm/gold/utility/powerPlay/_checkpoints/\"\n",
    "    }\n",
    "}\n",
    "\n",
    "src_table_name          = settings.get(\"src_table_name\")\n",
    "dst_table_name          = settings.get(\"dst_table_name\")\n",
    "readStreamOptions       = settings.get(\"readStreamOptions\")\n",
    "writeStreamOptions      = settings.get(\"writeStreamOptions\")\n",
    "merge_condition         = settings.get(\"merge_condition\")\n",
    "\n",
    "def upsert_to_gold(microBatchDF, batchId):\n",
    "    microBatchDF = microBatchDF.withColumn(\"created_on\", col(\"ingest_time\"))\n",
    "    microBatchDF = microBatchDF.withColumn(\"deleted_on\", lit(None).cast(\"timestamp\"))\n",
    "    microBatchDF = microBatchDF.withColumn(\"current_flag\", lit(\"Yes\"))\n",
    "    microBatchDF = microBatchDF.withColumn(\"valid_from\", col(\"ingest_time\"))\n",
    "    microBatchDF = microBatchDF.withColumn(\"valid_to\", lit(\"9999-12-31 23:59:59\").cast(\"timestamp\"))\n",
    "\n",
    "    ignore_cols = (\n",
    "        \"date\", \"ingest_time\", \"file_path\", \"file_modification_time\", \"source_metadata\",\n",
    "        \"created_on\", \"deleted_on\", \"current_flag\", \"valid_from\", \"valid_to\"\n",
    "    )\n",
    "    microBatchDF = microBatchDF.withColumn(\n",
    "        \"row_hash\",\n",
    "        sha2(to_json(struct(*[col(c) for c in microBatchDF.columns if c not in ignore_cols])),256)\n",
    "    )\n",
    "\n",
    "    # Sanity check\n",
    "    create_table_if_not_exists(spark, microBatchDF, dst_table_name)\n",
    "    \n",
    "    microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {dst_table_name} t\n",
    "        USING updates s\n",
    "        ON {merge_condition} AND t.current_flag='Yes'\n",
    "        WHEN MATCHED AND t.row_hash<>s.row_hash THEN\n",
    "            UPDATE SET\n",
    "                t.deleted_on=s.ingest_time,\n",
    "                t.current_flag='No',\n",
    "                t.valid_to=s.ingest_time\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {dst_table_name}\n",
    "        SELECT\n",
    "            s.* EXCEPT (current_flag, deleted_on, valid_from, created_on, valid_to),\n",
    "            s.ingest_time AS created_on,\n",
    "            NULL AS deleted_on,\n",
    "            'Yes' AS current_flag,\n",
    "            s.ingest_time AS valid_from,\n",
    "            CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS valid_to\n",
    "        FROM updates s\n",
    "        LEFT JOIN {dst_table_name} t\n",
    "            ON {merge_condition} AND t.current_flag='Yes'\n",
    "        WHERE t.current_flag IS NULL\n",
    "    \"\"\")\n",
    "\n",
    "history_df = DeltaTable.forName(spark, src_table_name).history()\n",
    "versions   = [r.version for r in history_df.select(\"version\").collect()]\n",
    "\n",
    "spark.sql(f\"TRUNCATE TABLE {dst_table_name}\")\n",
    "for v in sorted(versions):\n",
    "    batchDF = (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .option(\"versionAsOf\", v)\n",
    "        .table(src_table_name)\n",
    "    )\n",
    "    upsert_to_gold(batchDF, None)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_reconstruct_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
