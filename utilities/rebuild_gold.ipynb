{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7e4a85-e723-4860-af50-934425f0a5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This ugly crap is needed to import modules from the parent folder\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(f\"{os.getcwd()}/..\")\n",
    "\n",
    "from pyspark.sql.functions import to_date, from_utc_timestamp, current_timestamp, lit, sha2, to_json, struct, col\n",
    "from functions import create_table_if_not_exists\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "settings = {\n",
    "    \"pipeline_function\": \"gold.powerPlay\",\n",
    "    \"src_table_name\": \"edsm.silver.powerPlay\",\n",
    "    \"dst_table_name\": \"edsm.gold.powerPlay\",\n",
    "    \"merge_condition\": \"t.name = s.name and t.id = s.id and t.id64 = s.id64 and t.power = s.power\",\n",
    "    \"readStreamOptions\": {\n",
    "        \"rescuedDataColumn\": \"_rescued_data\",\n",
    "        \"ignoreChanges\": \"true\"\n",
    "    },\n",
    "    \"writeStreamOptions\": {\n",
    "        \"checkpointLocation\": \"/Volumes/edsm/gold/utility/powerPlay/_checkpoints/\"\n",
    "    }\n",
    "}\n",
    "\n",
    "src_table_name          = settings.get(\"src_table_name\")\n",
    "dst_table_name          = settings.get(\"dst_table_name\")\n",
    "readStreamOptions       = settings.get(\"readStreamOptions\")\n",
    "writeStreamOptions      = settings.get(\"writeStreamOptions\")\n",
    "merge_condition         = settings.get(\"merge_condition\")\n",
    "\n",
    "def upsert_to_gold(microBatchDF, batchId):\n",
    "    microBatchDF = microBatchDF.withColumn(\"created_on\", col(\"ingest_time\"))\n",
    "    microBatchDF = microBatchDF.withColumn(\"deleted_on\", lit(None).cast(\"timestamp\"))\n",
    "    microBatchDF = microBatchDF.withColumn(\"current_flag\", lit(\"Yes\"))\n",
    "    microBatchDF = microBatchDF.withColumn(\"valid_from\", col(\"ingest_time\"))\n",
    "    microBatchDF = microBatchDF.withColumn(\"valid_to\", lit(\"9999-12-31 23:59:59\").cast(\"timestamp\"))\n",
    "\n",
    "    fields_to_hash = [\"id\", \"id64\", \"name\", \"power\", \"powerState\", \"state\"]\n",
    "    microBatchDF = microBatchDF.withColumn(\n",
    "        \"row_hash\",\n",
    "        sha2(to_json(struct(*[col(c) for c in fields_to_hash])),256)\n",
    "    )\n",
    "    microBatchDF = microBatchDF.dropDuplicates([\"id\", \"id64\", \"name\", \"power\", \"row_hash\"])\n",
    "\n",
    "    dupes = microBatchDF.groupBy(fields_to_hash).count().filter(\"count > 1\")\n",
    "    dupes.show(truncate=False)\n",
    "    print(dupes.count())\n",
    "\n",
    "    # Sanity check\n",
    "    create_table_if_not_exists(spark, microBatchDF, dst_table_name)\n",
    "    \n",
    "    microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {dst_table_name} t\n",
    "        USING updates s\n",
    "        ON {merge_condition} AND t.current_flag='Yes'\n",
    "        WHEN MATCHED AND t.row_hash<>s.row_hash THEN\n",
    "            UPDATE SET\n",
    "                t.deleted_on=s.ingest_time,\n",
    "                t.current_flag='No',\n",
    "                t.valid_to=s.ingest_time\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {dst_table_name}\n",
    "        SELECT\n",
    "            s.* EXCEPT (current_flag, deleted_on, valid_from, created_on, valid_to),\n",
    "            s.ingest_time AS created_on,\n",
    "            NULL AS deleted_on,\n",
    "            'Yes' AS current_flag,\n",
    "            s.ingest_time AS valid_from,\n",
    "            CAST('9999-12-31 23:59:59' AS TIMESTAMP) AS valid_to\n",
    "        FROM updates s\n",
    "        LEFT JOIN {dst_table_name} t\n",
    "            ON {merge_condition} AND t.current_flag='Yes'\n",
    "        WHERE t.current_flag IS NULL\n",
    "    \"\"\")\n",
    "\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {src_table_name}\").select(\"version\").orderBy(\"version\").collect()\n",
    "spark.sql(f\"TRUNCATE TABLE {dst_table_name}\")\n",
    "for i in range(len(history_df) - 1):\n",
    "    start_version = history_df[i]['version']\n",
    "    end_version = history_df[i+1]['version']\n",
    "    batchDF = (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .option(\"readChangeData\", \"true\")\n",
    "        .option(\"startingVersion\", start_version)\n",
    "        .option(\"endingVersion\", end_version)\n",
    "        .table(src_table_name)\n",
    "    )\n",
    "    if batchDF.count() > 0:\n",
    "        upsert_to_gold(batchDF, None)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rebuild_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
