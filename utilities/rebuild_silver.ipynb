{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb7e4a85-e723-4860-af50-934425f0a5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This ugly crap is needed to import modules from the parent folder\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(f\"{os.getcwd()}/..\")\n",
    "\n",
    "import json\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import struct, to_json, sha2, col, current_timestamp\n",
    "from pyspark.sql.functions import to_timestamp, concat, regexp_extract, lit, date_format\n",
    "from functions import create_table_if_not_exists, rename_columns, cast_data_types\n",
    "\n",
    "settings = {\n",
    "    \"pipeline_function\": \"silver.powerPlay\",\n",
    "    \"src_table_name\": \"edsm.bronze.powerPlay\",\n",
    "    \"dst_table_name\": \"edsm.silver.powerPlay\",\n",
    "    \"build_history\": \"true\",\n",
    "    \"merge_condition\": \"t.name = s.name and t.id = s.id and t.id64 = s.id64 and t.power = s.power\",\n",
    "    \"data_type_map\": {\n",
    "        \"date\": \"timestamp\"\n",
    "    },\n",
    "    \"readStreamOptions\": {\n",
    "        \"rescuedDataColumn\": \"_rescued_data\",\n",
    "        \"ignoreChanges\": \"true\"\n",
    "    },\n",
    "    \"writeStreamOptions\": {\n",
    "        \"mergeSchema\": \"true\",\n",
    "        \"checkpointLocation\": \"/Volumes/edsm/silver/utility/powerPlay/_checkpoints/\",\n",
    "        \"delta.columnMapping.mode\": \"name\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Variables (json file)\n",
    "src_table_name              = settings.get(\"src_table_name\")\n",
    "dst_table_name              = settings.get(\"dst_table_name\")\n",
    "readStreamOptions           = settings.get(\"readStreamOptions\")\n",
    "writeStreamOptions          = settings.get(\"writeStreamOptions\")\n",
    "pk_name                     = settings.get(\"pk\", {}).get(\"name\")\n",
    "pk_columns                  = settings.get(\"pk\", {}).get(\"columns\")\n",
    "merge_condition             = settings.get(\"merge_condition\")\n",
    "column_map                  = settings.get(\"column_map\")\n",
    "data_type_map               = settings.get(\"data_type_map\")\n",
    "\n",
    "def upsert_to_silver(microBatchDF, batchId):\n",
    "    microBatchDF = (\n",
    "        microBatchDF.transform(rename_columns, column_map)\n",
    "        .transform(cast_data_types, data_type_map)\n",
    "        .withColumn(\"file_path\", col(\"source_metadata\").getField(\"file_path\"))\n",
    "        .withColumn(\"file_modification_time\", col(\"source_metadata\").getField(\"file_modification_time\"))\n",
    "        .withColumn(\n",
    "            \"ingest_time\",\n",
    "            to_timestamp(\n",
    "                concat(\n",
    "                    regexp_extract(col(\"source_metadata.file_path\"), \"/landing/(\\\\d{8})/\", 1),\n",
    "                    lit(\" \"),\n",
    "                    date_format(current_timestamp(), \"HH:mm:ss\"),\n",
    "                ),\n",
    "                \"yyyyMMdd HH:mm:ss\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fields_to_hash = [\"id\", \"id64\", \"name\", \"power\", \"powerState\", \"state\"]\n",
    "    microBatchDF = microBatchDF.withColumn(\n",
    "        \"row_hash\",\n",
    "        sha2(to_json(struct(*[col(c) for c in fields_to_hash])),256)\n",
    "    )\n",
    "\n",
    "    # Sanity check\n",
    "    create_table_if_not_exists(spark, microBatchDF, dst_table_name)\n",
    "\n",
    "    microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "    spark.sql(\n",
    "        f\"\"\"\n",
    "        MERGE INTO {dst_table_name} t\n",
    "        USING updates s\n",
    "        ON {merge_condition}\n",
    "        WHEN MATCHED AND t.row_hash<>s.row_hash THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "spark.sql(f\"TRUNCATE TABLE {dst_table_name}\")\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {src_table_name}\").select(\"version\").orderBy(\"version\").collect()\n",
    "\n",
    "for i in range(len(history) - 1):\n",
    "    start_version = history[i]['version']\n",
    "    end_version = history[i+1]['version']\n",
    "    changes = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .option(\"readChangeData\", \"true\")\n",
    "        .option(\"startingVersion\", start_version)\n",
    "        .option(\"endingVersion\", end_version)\n",
    "        .table(src_table_name)\n",
    "    )\n",
    "    if changes.count() > 0:\n",
    "        upsert_to_silver(changes, None)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "rebuild_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
