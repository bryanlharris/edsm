{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c024346b-7af1-4838-9625-3d9006333016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from functions import get_function\n",
    "\n",
    "# Variables\n",
    "color                       = dbutils.widgets.get(\"color\")\n",
    "job_settings                = json.loads(dbutils.widgets.get(\"job_settings\"))\n",
    "table                       = job_settings['table']\n",
    "settings                    = json.loads(next(Path().glob(f\"./layer_*_{color}/{table}.json\")).read_text())\n",
    "dst_table_name              = settings[\"dst_table_name\"]\n",
    "\n",
    "# Print job and table settings\n",
    "settings_message = f\"\\n\\nDictionary from {color}_settings.json:\\n\\n\"\n",
    "settings_message += json.dumps(job_settings, indent=4)\n",
    "settings_message += f\"\\n\\nContents of {table}.json:\\n\\n\"\n",
    "settings_message += json.dumps(settings, indent=4)\n",
    "print(settings_message)\n",
    "\n",
    "# One function for pipeline\n",
    "if \"pipeline_function\" in settings:\n",
    "    pipeline_function = get_function(settings[\"pipeline_function\"])\n",
    "    pipeline_function(spark, settings)\n",
    "\n",
    "# Individual functions for each step\n",
    "elif all(k in settings for k in [\"read_function\", \"transform_function\", \"write_function\"]):\n",
    "    read_function = get_function(settings[\"read_function\"])\n",
    "    transform_function = get_function(settings[\"transform_function\"])\n",
    "    write_function = get_function(settings[\"write_function\"])\n",
    "\n",
    "    df = read_function(spark, settings)\n",
    "    df = transform_function(spark, settings, df)\n",
    "    write_function(spark, settings, df)\n",
    "else:\n",
    "    raise Exception(f\"Could not find any ingest function name in settings.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
