{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "nuid": "00000000-0000-0000-0000-000000000000",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": "",
          "inputWidgets": {},
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          }
        }
      },
      "outputs": [],
      "source": [
        "def delete_ingest_records(spark, catalog, date='20250724'):\n",
        "    spark.sql(f'use catalog {catalog}')\n",
        "    system_schemas = ['information_schema', 'sys']\n",
        "    schemas = [row.databaseName for row in spark.sql(f'SHOW SCHEMAS IN {catalog}').collect()]\n",
        "    for schema in schemas:\n",
        "        if schema in system_schemas:\n",
        "            print(f'Skipping system schema {schema}')\n",
        "            continue\n",
        "        tables = spark.sql(f'SHOW TABLES IN {catalog}.{schema}').filter('isTemporary = false')\n",
        "        view_rows = spark.sql(f'SHOW VIEWS IN {catalog}.{schema}').collect()\n",
        "        views = {row.viewName for row in view_rows}\n",
        "        for row in tables.collect():\n",
        "            name = row.tableName\n",
        "            if name in views:\n",
        "                print(f'Skipping view {catalog}.{schema}.{name}')\n",
        "                continue\n",
        "            table_ref = f'{catalog}.{schema}.{name}'\n",
        "            cols = spark.table(table_ref).columns\n",
        "            conditions = []\n",
        "            if 'ingest_time' in cols:\n",
        "                conditions.append(f\"ingest_time = '{date}'\")\n",
        "            if 'derived_ingest_time' in cols:\n",
        "                conditions.append(f\"derived_ingest_time = '{date}'\")\n",
        "            if not conditions:\n",
        "                print(f'Skipping {table_ref}: no ingest columns')\n",
        "                continue\n",
        "            where_clause = ' OR '.join(conditions)\n",
        "            spark.sql(f'DELETE FROM {table_ref} WHERE {where_clause}')\n",
        "            print(f'Deleted records from {table_ref}')\n",
        "\n",
        "delete_ingest_records(spark, 'edsm')\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "3"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "delete_ingest_20250724",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}