{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install databricks-labs-dqx==0.6.0"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from functions.utility import apply_job_type, print_settings, schema_exists\n",
    "from functions.history import build_and_merge_file_history\n",
    "\n",
    "color = dbutils.widgets.get('color')\n",
    "job_settings = json.loads(dbutils.widgets.get('job_settings'))\n",
    "table = job_settings['table']\n",
    "settings = json.loads(next(Path().glob(f'./layer_*_{color}/{table}.json')).read_text())\n",
    "settings = apply_job_type(settings)\n",
    "dst_table_name = settings['dst_table_name']\n",
    "\n",
    "print_settings(job_settings, settings, color, table)\n",
    "\n",
    "history = job_settings.get('history', {})\n",
    "if str(history.get('build_history', 'false')).lower() == 'true':\n",
    "    full_table_name = history.get('full_table_name', dst_table_name)\n",
    "    history_schema = history.get('history_schema')\n",
    "    catalog = full_table_name.split('.')[0]\n",
    "    if history_schema is None:\n",
    "        print('Skipping history build: no history_schema provided')\n",
    "    elif schema_exists(catalog, history_schema, spark):\n",
    "        build_and_merge_file_history(full_table_name, history_schema, spark)\n",
    "    else:\n",
    "        print(f'Skipping history build: schema {catalog}.{history_schema} not found')\n"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}