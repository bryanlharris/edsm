{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ca54754c-332e-4e52-ae3b-50f40ddc6684",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from functions.sanity import validate_settings, initialize_empty_tables\n",
        "from functions.job import save_job_configuration\n",
        "from functions.utility import apply_job_type\n",
        "from functions.s3_utils import upload_file\n",
        "\n",
        "from functions.config import PROJECT_ROOT\n",
        "\n",
        "catalog = dbutils.widgets.get(\"catalog\")\n",
        "settings_s3_prefix = dbutils.widgets.get(\"settings_s3_prefix\")\n",
        "\n",
        "\n",
        "# Upload settings files and build task values\n",
        "job_settings = {}\n",
        "for color in [ 'bronze', 'silver', 'gold' ]:\n",
        "    paths = glob(f'./layer_*_{color}/*.json')\n",
        "    entries = []\n",
        "    for p in paths:\n",
        "        table = Path(p).stem\n",
        "        s3_uri = f\"{settings_s3_prefix}/{color}/{Path(p).name}\"\n",
        "        upload_file(p, s3_uri, dbutils)\n",
        "        entries.append({'table': table, 'settings_uri': s3_uri})\n",
        "    job_settings[color] = entries\n",
        "for key, value in job_settings.items():\n",
        "    dbutils.jobs.taskValues.set(key=key, value=value)\n",
        "\n",
        "# Load anything in layer_*/*.json if it has a key \"dst_table_name\"\n",
        "history_settings = []\n",
        "for p in glob('./layer_*/*.json'):\n",
        "    with open(p) as f:\n",
        "        settings = json.load(f)\n",
        "        settings = apply_job_type(settings)\n",
        "        if str(settings.get('build_history', 'false')).lower() == 'true':\n",
        "            entry = {'full_table_name': settings['dst_table_name']}\n",
        "            if 'history_schema' in settings:\n",
        "                entry['history_schema'] = settings['history_schema']\n",
        "            history_settings.append(entry)\n",
        "dbutils.jobs.taskValues.set(key='history_settings', value=history_settings)\n",
        "\n",
        "# Sanity check\n",
        "validate_settings(dbutils)\n",
        "initialize_empty_tables(spark)\n",
        "\n",
        "# Save job configuration\n",
        "save_job_configuration(dbutils, f\"/Volumes/{catalog}/bronze/utility/jobs\")\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": {
        "hardware": {
          "accelerator": null,
          "gpuPoolId": null,
          "memory": null
        }
      },
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "2"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 4827367985281887,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "00_job_settings",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
