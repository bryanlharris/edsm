{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca54754c-332e-4e52-ae3b-50f40ddc6684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from functions.sanity import validate_settings, initialize_empty_tables\n",
    "from functions.job import save_job_configuration\n",
    "from functions.utility import apply_job_type\n",
    "\n",
    "try:\n",
    "    save_job_config = dbutils.widgets.get('save_job_config')\n",
    "except Exception:\n",
    "    save_job_config = 'false'\n",
    "save_job_config = str(save_job_config).lower()\n",
    "\n",
    "if save_job_config == 'true':\n",
    "    try:\n",
    "        catalog = dbutils.widgets.get('catalog')\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"The 'catalog' widget is required when saving the job configuration. Please set it with the Jobs & Workflows web interface before running this job.\") from e\n",
    "\n",
    "    try:\n",
    "        bronze_schema = dbutils.widgets.get('bronze_schema')\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"The 'bronze_schema' widget is required when saving the job configuration. Please set it with the Jobs & Workflows web interface before running this job.\") from e\n",
    "\n",
    "# Load anything in layer_*_<color>/*.json\n",
    "job_settings = {}\n",
    "for color in [ \"bronze\", \"silver\", \"gold\" ]:\n",
    "    paths = glob(f'./layer_*_{color}/*.json')\n",
    "    job_settings[color] = [\n        {\"table\": path.split(\"/\")[-1].split(\".\")[0]}\n        for path in paths]\n",
    "for key, value in job_settings.items():\n",
    "    dbutils.jobs.taskValues.set(key=key, value=value)\n",
    "\n",
    "# Load anything in layer_*/*.json if it has a key \"dst_table_name\"\n",
    "history_settings = []\n",
    "for p in glob('./layer_*/*.json'):\n",
    "    with open(p) as f:\n",
    "        settings = json.load(f)\n",
    "        settings = apply_job_type(settings)\n",
    "        if str(settings.get(\"build_history\", \"false\")).lower() == \"true\":\n",
    "            entry = {\"full_table_name\": settings[\"dst_table_name\"]}\n",
    "            if \"history_schema\" in settings:\n",
    "                entry[\"history_schema\"] = settings[\"history_schema\"]\n",
    "            history_settings.append(entry)\n",
    "dbutils.jobs.taskValues.set(key=\"history_settings\", value=history_settings)\n",
    "\n",
    "# Sanity check\n",
    "validate_settings(dbutils)\n",
    "initialize_empty_tables(spark)\n",
    "\n",
    "if save_job_config == 'true':\n",
    "    # Save job configuration\n",
    "    save_job_configuration(dbutils, f\"/Volumes/{catalog}/{bronze_schema}/utility/jobs\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4827367985281887,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_job_settings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
