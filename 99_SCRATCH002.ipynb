{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c0d825-9daa-4e6f-8c3b-9f9514bf9dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"pipeline_function\": \"temporal_write\",\n",
    "    \"src_table_name\": \"edsm.silver.powerPlay\",\n",
    "    \"dst_table_name\": \"edsm.gold.powerPlay\",\n",
    "    \"merge_columns\": [ \"name\", \"id\", \"id64\", \"power\" ],\n",
    "    \"pk2\": \"primary_key\",\n",
    "    \"readStreamOptions\": {\n",
    "        \"rescuedDataColumn\": \"_rescued_data\",\n",
    "        \"ignoreChanges\": \"true\"\n",
    "    },\n",
    "    \"writeStreamOptions\": {\n",
    "        \"mergeSchema\": \"true\",\n",
    "        \"checkpointLocation\": \"/Volumes/edsm/gold/utility/powerPlay/_checkpoints/\",\n",
    "        \"delta.columnMapping.mode\": \"name\"\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "from pyspark.sql.functions import current_timestamp, isnull, lit\n",
    "from functions import *\n",
    "\n",
    "# Variables (json file)\n",
    "src_table_name          = settings.get(\"src_table_name\")\n",
    "dst_table_name          = settings.get(\"dst_table_name\")\n",
    "readStreamOptions       = settings.get(\"readStreamOptions\")\n",
    "writeStreamOptions      = settings.get(\"writeStreamOptions\")\n",
    "pk                      = settings.get(\"pk\")\n",
    "pk2                     = settings.get(\"pk2\")\n",
    "merge_columns           = settings.get(\"merge_columns\")\n",
    "\n",
    "# Read\n",
    "# df = (\n",
    "#     spark.readStream\n",
    "#     .format(\"delta\")\n",
    "#     .options(**readStreamOptions)\n",
    "#     .table(src_table_name)\n",
    "# )\n",
    "\n",
    "df = spark.read.table(src_table_name)\n",
    "\n",
    "############################################\n",
    "### Your normal table transforms go here ###\n",
    "############################################\n",
    "\n",
    "##################################################\n",
    "### Some of these are for the temporal columns ###\n",
    "##################################################\n",
    "# Need: current_flag                             #\n",
    "# Need: created_on                               #\n",
    "# Need: deleted_on                               #\n",
    "##################################################\n",
    "df = (\n",
    "    df.toDF(*[c.lower() for c in df.columns])\n",
    "    .withColumn('current_flag', lit('Y'))\n",
    "    .withColumn(\"created_on\", current_timestamp())\n",
    "    .withColumn(\"deleted_on\", lit(None).cast(\"timestamp\"))\n",
    "    .withColumn(\"effective_dt\", current_timestamp())\n",
    ")\n",
    "\n",
    "################################################################\n",
    "### This creates a column called primary_key which is a hash ###\n",
    "################################################################\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "columns = [\n",
    "    f.name for f in df.schema.fields \n",
    "    if f.dataType.simpleString() in [ \"string\", \"int\", \"decimal\", \"date\", \"timestamp\" ]\n",
    "]\n",
    "columns.remove(\"current_flag\")\n",
    "columns.remove(\"created_on\")\n",
    "columns.remove(\"effective_dt\")\n",
    "coalesced = [f\"COALESCE(cast({col} as string), '')\" for col in columns]\n",
    "sqlString = \"SELECT *,\\n\" + \"  SHA2(CONCAT(\" + \", '-', \".join(coalesced) + \"), 256) as primary_key\\n\" + f\" FROM df\"\n",
    "df = spark.sql(sqlString)\n",
    "\n",
    "#######################################################\n",
    "### Rearrange the columns so the temporal are first ###\n",
    "#######################################################\n",
    "columns = df.columns\n",
    "columns.remove(\"current_flag\")\n",
    "columns.remove(\"created_on\")\n",
    "columns.remove(\"deleted_on\")\n",
    "columns = [\"current_flag\", \"created_on\", \"deleted_on\"] + columns\n",
    "\n",
    "df = df.select(*columns)\n",
    "\n",
    "# Sanity check\n",
    "create_table_if_not_exists(spark, df, dst_table_name)\n",
    "\n",
    "#######################################\n",
    "### These are to prevent duplicates ###\n",
    "#######################################\n",
    "# Need to ignore certain columns\n",
    "#\n",
    "# Ignore: current_flag\n",
    "# Ignore: created_on\n",
    "# Ignore: effective_dt\n",
    "#######################\n",
    "df.createOrReplaceTempView(\"a\")\n",
    "df1 = spark.table(dst_table_name)\n",
    "df1.createOrReplaceTempView(\"b\")\n",
    "\n",
    "new = spark.sql(f\"\"\"\n",
    "    select *\n",
    "        except (current_flag, created_on, effective_dt)\n",
    "    from a\n",
    "    except\n",
    "    select *\n",
    "        except (current_flag, created_on, effective_dt)\n",
    "    from b\n",
    "    where current_flag = \"Y\";\n",
    "\"\"\")\n",
    "new = new.select(*merge_columns)\n",
    "df = df.join(new, merge_columns, \"inner\")\n",
    "\n",
    "#######################################\n",
    "### Drop complete record duplicates ###\n",
    "#######################################\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "###########################\n",
    "### Append to the table ###\n",
    "###########################\n",
    "df.write.mode(\"append\").format(\"delta\").saveAsTable(dst_table_name)\n",
    "\n",
    "##############################################################################\n",
    "### This finds primary keys missing from the source side (deleted records) ###\n",
    "##############################################################################\n",
    "# pk: the numerical primary key from the source\n",
    "# pk2: the hash of all columns primary key\n",
    "##############################################################################\n",
    "df1 = spark.sql(f\"select * from {src_table_name}\")\n",
    "df2 = spark.sql(f\"select * from {dst_table_name}\")\n",
    "df2 = df2.join(df1, merge_columns, \"leftanti\")\n",
    "df2 = df2.filter((isnull(df2.deleted_on)) & (df2.current_flag == 'Y'))\n",
    "missing = df2.select(pk2)\n",
    "\n",
    "#####################################################\n",
    "### For every missing key, change certain columns ###\n",
    "#####################################################\n",
    "# Change: current_flag\n",
    "# Change: deleted_on\n",
    "# Change: effective_dt\n",
    "#####################################################\n",
    "deltaTable = DeltaTable.forName(spark, dst_table_name)\n",
    "delete = ( deltaTable.alias(\"d\")\n",
    "                    .merge(missing.alias(\"s\"), f\"s.{pk2} = d.{pk2}\")\n",
    "                    .whenMatchedUpdate(set = {\n",
    "                        \"current_flag\": lit(\"N\"),\n",
    "                        \"deleted_on\": current_timestamp(),\n",
    "                        \"effective_dt\": current_timestamp()\n",
    "                    })\n",
    "                    .execute()\n",
    "        )\n",
    "\n",
    "##############################################################################\n",
    "### This sets every active to \"N\" except the newest record per primary key ###\n",
    "##############################################################################\n",
    "key_expr = \",\".join(merge_columns)\n",
    "on_expr = \" AND \".join([f\"t.{k} = s.{k}\" for k in merge_columns])\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {dst_table_name} t\n",
    "    USING (\n",
    "        SELECT\n",
    "            {key_expr},\n",
    "            MAX(created_on) AS max_created_on\n",
    "        FROM {dst_table_name}\n",
    "        GROUP BY {key_expr}\n",
    "    ) s\n",
    "    ON {on_expr}\n",
    "    WHEN MATCHED AND t.created_on <> s.max_created_on THEN\n",
    "        UPDATE SET t.current_flag = 'N'\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "99_SCRATCH002",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
