{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c024346b-7af1-4838-9625-3d9006333016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from functions.s3_utils import read_text\n",
    "\n",
    "# Variables\n",
    "color = dbutils.widgets.get('color')\n",
    "job_settings = json.loads(dbutils.widgets.get('job_settings'))\n",
    "table = job_settings['table']\n",
    "settings_uri = job_settings.get('settings_uri')\n",
    "# Propagate AWS credentials so executors can authenticate\n",
    "access_key = dbutils.secrets.get('edsm', 'aws_access_key_id')\n",
    "secret_key = dbutils.secrets.get('edsm', 'aws_secret_access_key')\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
    "\n",
    "\n",
    "from functions.utility import get_function, create_bad_records_table, apply_job_type\n",
    "\n",
    "if settings_uri:\n",
    "    raw = read_text(settings_uri, dbutils)\n",
    "    settings = json.loads(raw)\n",
    "else:\n",
    "    settings_path = next(Path().glob(f'./layer_*_{color}/{table}.json'))\n",
    "    settings = json.loads(Path(settings_path).read_text())\n",
    "settings = apply_job_type(settings)\n",
    "\n",
    "dst_table_name = settings['dst_table_name']\n",
    "\n",
    "# Print job and table settings\n",
    "settings_message = f'\\n\\nDictionary from {color}_settings.json:\\n\\n'\n",
    "settings_message += json.dumps(job_settings, indent=4)\n",
    "settings_message += f'\\n\\nContents of {table}.json:\\n\\n'\n",
    "settings_message += json.dumps(settings, indent=4)\n",
    "print(settings_message)\n",
    "\n",
    "# One function for pipeline\n",
    "if 'pipeline_function' in settings:\n",
    "    pipeline_function = get_function(settings['pipeline_function'])\n",
    "    pipeline_function(settings, spark)\n",
    "\n",
    "# Individual functions for each step\n",
    "elif all(k in settings for k in ['read_function', 'transform_function', 'write_function']):\n",
    "    read_function = get_function(settings['read_function'])\n",
    "    transform_function = get_function(settings['transform_function'])\n",
    "    write_function = get_function(settings['write_function'])\n",
    "\n",
    "    df = read_function(settings, spark)\n",
    "    df = transform_function(df, settings, spark)\n",
    "    write_function(df, settings, spark)\n",
    "else:\n",
    "    raise Exception('Could not find any ingest function name in settings.')\n",
    "if color == 'bronze':\n",
    "    create_bad_records_table(settings, spark)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
