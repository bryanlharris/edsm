{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce22ba1-2df4-4875-8d4f-92389a35b1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "\n",
    "## Rebuild __init__.py files before getting started\n",
    "def extract_function_names(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        tree = ast.parse(f.read(), filename=filepath)\n",
    "    return [node.name for node in tree.body if isinstance(node, ast.FunctionDef)]\n",
    "\n",
    "def generate_init_file(directory):\n",
    "    lines = []\n",
    "    all_exports = []\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        fullpath = os.path.join(directory, filename)\n",
    "        if filename.endswith(\".py\") and filename != \"__init__.py\" and os.path.isfile(fullpath):\n",
    "            modname = filename[:-3]\n",
    "            func_names = extract_function_names(fullpath)\n",
    "            if func_names:\n",
    "                lines.append(f\"from .{modname} import \" + \", \".join(func_names))\n",
    "                all_exports.extend(func_names)\n",
    "    if all_exports:\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"__all__ = [\" + \", \".join(f'\"{name}\"' for name in all_exports) + \"]\")\n",
    "    init_path = os.path.join(directory, \"__init__.py\")\n",
    "    with open(init_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "for layer in [\"functions\", \"layer_01_bronze\", \"layer_02_silver\", \"layer_03_gold\", \"layer_04_history\"]:\n",
    "    generate_init_file(layer)\n",
    "print(\"Python __init__.py files have been rebuilt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1afb8b5-125f-4444-9d01-b779015db413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import importlib\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StructType\n",
    "from functions import create_table_if_not_exists\n",
    "\n",
    "\n",
    "## Check that all json settings files have the minimum required keys AKA functions before proceeding\n",
    "bronze_inputs=dbutils.jobs.taskValues.get(taskKey=\"job_settings\",key=\"bronze\")\n",
    "silver_inputs=dbutils.jobs.taskValues.get(taskKey=\"job_settings\",key=\"silver\")\n",
    "gold_inputs=dbutils.jobs.taskValues.get(taskKey=\"job_settings\",key=\"gold\")\n",
    "\n",
    "bronze_files={f.split(\"/\")[-1].replace(\".json\",\"\"): f for f in glob(\"./layer_*_bronze/*.json\")}\n",
    "silver_files={f.split(\"/\")[-1].replace(\".json\",\"\"): f for f in glob(\"./layer_*_silver/*.json\")}\n",
    "gold_files={f.split(\"/\")[-1].replace(\".json\",\"\"): f for f in glob(\"./layer_*_gold/*.json\")}\n",
    "\n",
    "all_tables = set(list(bronze_files.keys()) + list(silver_files.keys()) + list(gold_files.keys()))\n",
    "\n",
    "modules = {\n",
    "    \"bronze\": importlib.import_module(\"layer_01_bronze\"),\n",
    "    \"silver\": importlib.import_module(\"layer_02_silver\"),\n",
    "    \"gold\": importlib.import_module(\"layer_03_gold\")\n",
    "}\n",
    "\n",
    "layers=[\"bronze\",\"silver\",\"gold\"]\n",
    "required_functions={\n",
    "    \"bronze\":[\"read_function\",\"transform_function\",\"write_function\",\"dst_table_name\",\"file_schema\"],\n",
    "    \"silver\":[\"read_function\",\"transform_function\",\"write_function\",\"src_table_name\",\"dst_table_name\",\"composite_key\",\"business_key\",\"upsert_function\"],\n",
    "    \"gold\":[\"read_function\",\"transform_function\",\"write_function\",\"src_table_name\",\"dst_table_name\",\"composite_key\",\"business_key\"]\n",
    "}\n",
    "\n",
    "errs=[]\n",
    "\n",
    "# Check for required functions\n",
    "for layer, files in [(\"bronze\", bronze_files), (\"silver\", silver_files), (\"gold\", gold_files)]:\n",
    "    for tbl, path in files.items():\n",
    "        settings=json.loads(open(path).read())\n",
    "        for k in required_functions[layer]:\n",
    "            if k not in settings:\n",
    "                errs.append(f\"{path} missing {k}\")\n",
    "\n",
    "\n",
    "## For each table and each layer, cascade transforms and create table\n",
    "for tbl in sorted(all_tables):\n",
    "    df=None\n",
    "    skip_table=False\n",
    "    for layer in layers:\n",
    "        if layer==\"bronze\" and tbl not in bronze_files:\n",
    "            break\n",
    "        if layer==\"silver\" and tbl not in silver_files:\n",
    "            break\n",
    "        if layer==\"gold\" and tbl not in gold_files:\n",
    "            break\n",
    "        if layer==\"bronze\":\n",
    "            path=bronze_files[tbl]\n",
    "        elif layer==\"silver\":\n",
    "            path=silver_files[tbl]\n",
    "        elif layer==\"gold\":\n",
    "            path=gold_files[tbl]\n",
    "        settings=json.loads(open(path).read())\n",
    "        if layer==\"bronze\":\n",
    "            settings[\"use_metadata\"] = \"false\"\n",
    "            if \"file_schema\" not in settings:\n",
    "                errs.append(f\"{path} missing file_schema, cannot create table\")\n",
    "                skip_table=True\n",
    "                break\n",
    "            schema=StructType.fromJson(settings[\"file_schema\"])\n",
    "            df=spark.createDataFrame([], schema)\n",
    "        try:\n",
    "            modname, funcname=settings[\"transform_function\"].split(\".\")\n",
    "            transform_function=getattr(modules[layer], funcname)\n",
    "        except Exception:\n",
    "            errs.append(f\"{path} missing transform_function for {layer}, cannot create table\")\n",
    "            skip_table=True\n",
    "            break\n",
    "        df=transform_function(spark, settings, df)\n",
    "        dst=settings[\"dst_table_name\"]\n",
    "        if not spark.catalog.tableExists(dst):\n",
    "            create_table_if_not_exists(spark, df, dst)\n",
    "            print(f\"Table did not exist and was created: {dst}.\")\n",
    "    if skip_table:\n",
    "        continue\n",
    "\n",
    "if errs:\n",
    "    raise RuntimeError(\"Sanity check failed: \"+\", \".join(errs))\n",
    "else:\n",
    "    print(\"Sanity check passed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_sanity_check",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
