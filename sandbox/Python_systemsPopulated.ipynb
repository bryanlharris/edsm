{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42975e01-af12-42e6-9b79-5c4fbc40c55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(f\"{os.getcwd()}/..\")\n",
    "\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "from pyspark.sql.functions import concat, regexp_extract, date_format, current_timestamp\n",
    "from pyspark.sql.functions import when, col, to_timestamp, to_date, regexp_replace\n",
    "from pyspark.sql.functions import sha2, concat_ws, coalesce, lit, trim, struct\n",
    "from pyspark.sql.functions import to_json, expr, to_utc_timestamp\n",
    "from pyspark.sql.types import StructType, ArrayType\n",
    "from pyspark.sql.functions import transform\n",
    "import re\n",
    "from functions.utility import create_table_if_not_exists, get_function\n",
    "from functions.transform import silver_scd2_transform\n",
    "from functions.read import read_table\n",
    "from functions.write import batch_scd2_write\n",
    "\n",
    "catalog = dbutils.widgets.get('catalog')\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "settings = {\n",
  "    \"read_function\": \"functions.read.read_table\",\n",
  "    \"transform_function\": \"functions.transform.silver_scd2_transform\",\n",
  "    \"write_function\": \"functions.write.batch_scd2_write\",\n",
    "    \"src_table_name\": \"edsm.bronze.systemsPopulated\",\n",
    "    \"dst_table_name\": \"edsm.silver.systemsPopulated\",\n",
    "    \"build_history\": \"false\",\n",
    "    \"business_key\": [\n",
    "        \"id\"\n",
    "    ],\n",
    "    \"surrogate_key\": [\n",
    "        \"allegiance\",\n",
    "        \"controllingFaction\",\n",
    "        \"economy\",\n",
    "        \"factions\",\n",
    "        \"government\",\n",
    "        \"population\",\n",
    "        \"security\",\n",
    "        \"state\",\n",
    "        \"stations\"\n",
    "    ],\n",
    "    \"use_row_hash\": True,\n",
    "    \"row_hash_col\": \"row_hash\",\n",
    "    \"data_type_map\": {\n",
    "        \"date\": \"timestamp\"\n",
    "    },\n",
    "    \"readStreamOptions\": {\n",
    "        \"startingVersion\": 5\n",
    "    },\n",
    "    \"writeStreamOptions\": {\n",
    "        \"mergeSchema\": \"false\",\n",
    "        \"checkpointLocation\": f\"/Volumes/{catalog}/silver/utility/systemsPopulated/_checkpoints/\",\n",
    "        \"delta.columnMapping.mode\": \"name\"\n",
    "    },\n",
    "    \"ingest_time_column\": \"derived_ingest_time\"\n",
    "}\n",
    "\n",
    "df = read_table(spark, settings)\n",
    "df = silver_scd2_transform(df, settings, spark)\n",
    "\n",
    "df.select(\"row_hash\").limit(10).show()\n",
    "\n",
    "batch_scd2_write(df, settings, spark)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5443261718718942,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Python_systemsPopulated",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
