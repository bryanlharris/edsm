{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1afb8b5-125f-4444-9d01-b779015db413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "bronze_inputs=json.loads(dbutils.jobs.taskValues.get(taskKey=\"job_settings\",key=\"bronze\"))\n",
    "silver_inputs=json.loads(dbutils.jobs.taskValues.get(taskKey=\"job_settings\",key=\"silver\"))\n",
    "gold_inputs=json.loads(dbutils.jobs.taskValues.get(taskKey=\"job_settings\",key=\"gold\"))\n",
    "layer_files={\n",
    "    \"bronze\":[f for d in bronze_inputs for f in glob(f\"./layer_*_bronze/{d['table']}.json\")],\n",
    "    \"silver\":[f for d in silver_inputs for f in glob(f\"./layer_*_silver/{d['table']}.json\")],\n",
    "    \"gold\":[f for d in gold_inputs for f in glob(f\"./layer_*_gold/{d['table']}.json\")]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1de426fb-e38a-4ca2-9cd9-b0dd120f8d9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.types import StructType\n",
    "layers=[\"bronze\",\"silver\",\"gold\"]\n",
    "required={\n",
    "    \"bronze\":[\"read_function\",\"transform_function\",\"write_function\",\"dst_table_name\",\"file_schema\"],\n",
    "    \"silver\":[\"read_function\",\"transform_function\",\"write_function\",\"src_table_name\",\"dst_table_name\",\"composite_key\",\"business_key\"],\n",
    "    \"gold\":[\"read_function\",\"transform_function\",\"write_function\",\"src_table_name\",\"dst_table_name\",\"composite_key\",\"business_key\"]\n",
    "}\n",
    "errs=[]\n",
    "for layer in layers:\n",
    "    for path in layer_files[layer]:\n",
    "        cfg=json.loads(open(path).read())\n",
    "        for k in required[layer]:\n",
    "            if k not in cfg:\n",
    "                errs.append(f\"{path} missing {k}\")\n",
    "for layer in layers:\n",
    "    for path in layer_files[layer]:\n",
    "        cfg=json.loads(open(path).read())\n",
    "        dst=cfg[\"dst_table_name\"]\n",
    "        if not spark.catalog.tableExists(dst):\n",
    "            errs.append(f\"{layer} table {dst} not found\")\n",
    "if errs:\n",
    "    raise RuntimeError(\"Sanity check failed: \"+\", \".join(errs))\n",
    "else:\n",
    "    print(\"Sanity check passed\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sanity_check",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
